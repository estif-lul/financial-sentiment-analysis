{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4738a888",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:1296\u001b[39m\n\u001b[32m   1292\u001b[39m     rcParams[\u001b[33m'\u001b[39m\u001b[33mbackend_fallback\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_backend\u001b[39m(*, auto_select=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1300\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[33;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[32m   1302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1320\u001b[39m \u001b[33;03m    matplotlib.use\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:771\u001b[39m, in \u001b[36mRcParams.__setitem__\u001b[39m\u001b[34m(self, key, val)\u001b[39m\n\u001b[32m    769\u001b[39m         cval = \u001b[38;5;28mself\u001b[39m.validate[key](val)\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28mself\u001b[39m._set(key, cval)\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mValueError\u001b[39m: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffea1044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\est_lul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt', download_dir='C:\\\\nltk_data')\n",
    "nltk.download('stopwords', download_dir='C:\\\\nltk_data')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa6cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the parquet file\n",
    "    news_df = pd.read_parquet('../../data/raw_analyst_ratings.parquet')\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please ensure the path is correct and the file exists.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the parquet file: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8d9674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Length Statistics:\n",
      " {'Mean Length (Words)': np.float64(11.416705984674504), 'Median Length (Words)': np.float64(10.0), 'Standard Deviation': np.float64(6.352995020898485), 'Max Length': np.int64(77), 'Min Length': np.int64(1), 'Skewness': np.float64(2.1671569878139527), 'Kurtosis': np.float64(7.132395151646767)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>headline_length_chars</th>\n",
       "      <th>headline_length_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>957975</th>\n",
       "      <td>962820</td>\n",
       "      <td>Traders Attributing Strength In Orexigen To A ...</td>\n",
       "      <td>https://www.benzinga.com/movers/17/01/8864191/...</td>\n",
       "      <td>Paul Quintaro</td>\n",
       "      <td>2017-01-04 00:00:00+00:00</td>\n",
       "      <td>OREX</td>\n",
       "      <td>123</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618254</th>\n",
       "      <td>621448</td>\n",
       "      <td>HNI Corp. Sees Q2 Adj. EPS $0.54-$0.59 vs $0.4...</td>\n",
       "      <td>https://www.benzinga.com/news/16/04/7870130/hn...</td>\n",
       "      <td>Paul Quintaro</td>\n",
       "      <td>2016-04-21 00:00:00+00:00</td>\n",
       "      <td>HNI</td>\n",
       "      <td>93</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366076</th>\n",
       "      <td>1372510</td>\n",
       "      <td>Weight Watchers Guides FY 2011 EPS $3.75-4.00 ...</td>\n",
       "      <td>https://www.benzinga.com/news/guidance/11/05/1...</td>\n",
       "      <td>Benzinga Staff</td>\n",
       "      <td>2011-05-06 00:00:00+00:00</td>\n",
       "      <td>WTW</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390361</th>\n",
       "      <td>1396843</td>\n",
       "      <td>Groupon Could Target Online-Review Company Yel...</td>\n",
       "      <td>https://www.benzinga.com/m-a/19/09/14418038/gr...</td>\n",
       "      <td>Charles Gross</td>\n",
       "      <td>2019-09-11 00:00:00+00:00</td>\n",
       "      <td>YELP</td>\n",
       "      <td>121</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297645</th>\n",
       "      <td>299350</td>\n",
       "      <td>Earnings Scheduled For February 18, 2016</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/16/02/6...</td>\n",
       "      <td>Monica Gerson</td>\n",
       "      <td>2016-02-18 00:00:00+00:00</td>\n",
       "      <td>CRMT</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324780</th>\n",
       "      <td>326563</td>\n",
       "      <td>Feltl and Company Initiates Coverage on Cypres...</td>\n",
       "      <td>https://www.benzinga.com/analyst-ratings/price...</td>\n",
       "      <td>Juan Lopez</td>\n",
       "      <td>2012-08-28 00:00:00+00:00</td>\n",
       "      <td>CY</td>\n",
       "      <td>89</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387779</th>\n",
       "      <td>389865</td>\n",
       "      <td>63 Biggest Movers From Yesterday</td>\n",
       "      <td>https://www.benzinga.com/news/19/11/14855313/6...</td>\n",
       "      <td>Lisa Levin</td>\n",
       "      <td>2019-11-21 00:00:00+00:00</td>\n",
       "      <td>DVAX</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082380</th>\n",
       "      <td>1087766</td>\n",
       "      <td>Mid-Morning Market Update: Markets Mixed; Deer...</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/14/11/5...</td>\n",
       "      <td>Garrett Cook</td>\n",
       "      <td>2014-11-26 00:00:00+00:00</td>\n",
       "      <td>RIG</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89640</th>\n",
       "      <td>90422</td>\n",
       "      <td>Mid-Day Market Update: Crude Oil Up Over 4%; E...</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/16/12/8...</td>\n",
       "      <td>Lisa Levin</td>\n",
       "      <td>2016-12-01 00:00:00+00:00</td>\n",
       "      <td>ARDM</td>\n",
       "      <td>80</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94085</th>\n",
       "      <td>94881</td>\n",
       "      <td>Social Media Outlook for Friday May 11 (NVDA, ...</td>\n",
       "      <td>https://www.benzinga.com/news/earnings/12/05/2...</td>\n",
       "      <td>Social Market Analytics</td>\n",
       "      <td>2012-05-11 00:00:00+00:00</td>\n",
       "      <td>ARNA</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                           headline  \\\n",
       "957975       962820  Traders Attributing Strength In Orexigen To A ...   \n",
       "618254       621448  HNI Corp. Sees Q2 Adj. EPS $0.54-$0.59 vs $0.4...   \n",
       "1366076     1372510  Weight Watchers Guides FY 2011 EPS $3.75-4.00 ...   \n",
       "1390361     1396843  Groupon Could Target Online-Review Company Yel...   \n",
       "297645       299350           Earnings Scheduled For February 18, 2016   \n",
       "324780       326563  Feltl and Company Initiates Coverage on Cypres...   \n",
       "387779       389865                   63 Biggest Movers From Yesterday   \n",
       "1082380     1087766  Mid-Morning Market Update: Markets Mixed; Deer...   \n",
       "89640         90422  Mid-Day Market Update: Crude Oil Up Over 4%; E...   \n",
       "94085         94881  Social Media Outlook for Friday May 11 (NVDA, ...   \n",
       "\n",
       "                                                       url  \\\n",
       "957975   https://www.benzinga.com/movers/17/01/8864191/...   \n",
       "618254   https://www.benzinga.com/news/16/04/7870130/hn...   \n",
       "1366076  https://www.benzinga.com/news/guidance/11/05/1...   \n",
       "1390361  https://www.benzinga.com/m-a/19/09/14418038/gr...   \n",
       "297645   https://www.benzinga.com/news/earnings/16/02/6...   \n",
       "324780   https://www.benzinga.com/analyst-ratings/price...   \n",
       "387779   https://www.benzinga.com/news/19/11/14855313/6...   \n",
       "1082380  https://www.benzinga.com/news/earnings/14/11/5...   \n",
       "89640    https://www.benzinga.com/news/earnings/16/12/8...   \n",
       "94085    https://www.benzinga.com/news/earnings/12/05/2...   \n",
       "\n",
       "                       publisher                      date stock  \\\n",
       "957975             Paul Quintaro 2017-01-04 00:00:00+00:00  OREX   \n",
       "618254             Paul Quintaro 2016-04-21 00:00:00+00:00   HNI   \n",
       "1366076           Benzinga Staff 2011-05-06 00:00:00+00:00   WTW   \n",
       "1390361            Charles Gross 2019-09-11 00:00:00+00:00  YELP   \n",
       "297645             Monica Gerson 2016-02-18 00:00:00+00:00  CRMT   \n",
       "324780                Juan Lopez 2012-08-28 00:00:00+00:00    CY   \n",
       "387779                Lisa Levin 2019-11-21 00:00:00+00:00  DVAX   \n",
       "1082380             Garrett Cook 2014-11-26 00:00:00+00:00   RIG   \n",
       "89640                 Lisa Levin 2016-12-01 00:00:00+00:00  ARDM   \n",
       "94085    Social Market Analytics 2012-05-11 00:00:00+00:00  ARNA   \n",
       "\n",
       "         headline_length_chars  headline_length_words  \n",
       "957975                     123                     20  \n",
       "618254                      93                     17  \n",
       "1366076                     75                     11  \n",
       "1390361                    121                     17  \n",
       "297645                      40                      6  \n",
       "324780                      89                     14  \n",
       "387779                      32                      5  \n",
       "1082380                     80                     11  \n",
       "89640                       80                     14  \n",
       "94085                       62                     11  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date column to datetime format\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], format='mixed', utc=True)\n",
    "\n",
    "# Compute headline text length metrics (character and word count)\n",
    "news_df['headline_length_chars'] = news_df['headline'].apply(len)\n",
    "news_df['headline_length_words'] = news_df['headline'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Summary Statistics for Headline Length\n",
    "headline_stats = {\n",
    "    \"Mean Length (Words)\": np.mean(news_df['headline_length_words']),\n",
    "    \"Median Length (Words)\": np.median(news_df['headline_length_words']),\n",
    "    \"Standard Deviation\": np.std(news_df['headline_length_words']),\n",
    "    \"Max Length\": np.max(news_df['headline_length_words']),\n",
    "    \"Min Length\": np.min(news_df['headline_length_words']),\n",
    "    \"Skewness\": stats.skew(news_df['headline_length_words']),\n",
    "    \"Kurtosis\": stats.kurtosis(news_df['headline_length_words']),\n",
    "}\n",
    "print(\"Headline Length Statistics:\\n\", headline_stats)\n",
    "\n",
    "news_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd85d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yearly News Article Counts:\n",
      " year\n",
      "2009     11489\n",
      "2010     81319\n",
      "2011    131322\n",
      "2012    122649\n",
      "2013    121529\n",
      "2014    134859\n",
      "2015    135295\n",
      "2016    141892\n",
      "2017    124456\n",
      "2018    146924\n",
      "2019    150380\n",
      "2020    105214\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Monthly News Article Counts:\n",
      " month\n",
      "1     121545\n",
      "2     122836\n",
      "3     121949\n",
      "4     121813\n",
      "5     130340\n",
      "6     106598\n",
      "7     110764\n",
      "8     124041\n",
      "9      96089\n",
      "10    124800\n",
      "11    121430\n",
      "12    105123\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Hourly Distribution of News:\n",
      " hour\n",
      "0     1351472\n",
      "1          82\n",
      "2          48\n",
      "3          27\n",
      "4          67\n",
      "5          14\n",
      "6          57\n",
      "7          93\n",
      "8        1469\n",
      "9        1829\n",
      "10       2476\n",
      "11       5033\n",
      "12       5527\n",
      "13       5965\n",
      "14       7669\n",
      "15       5701\n",
      "16       5732\n",
      "17       2710\n",
      "18       2075\n",
      "19       1612\n",
      "20       3939\n",
      "21       2800\n",
      "22        704\n",
      "23        227\n",
      "Name: headline, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Temporal Analysis of News Publications (Yearly and Monthly Trends)\n",
    "news_df['year'] = news_df['date'].dt.year\n",
    "news_df['month'] = news_df['date'].dt.month\n",
    "\n",
    "yearly_counts = news_df.groupby('year')['headline'].count()\n",
    "monthly_counts = news_df.groupby('month')['headline'].count()\n",
    "\n",
    "# Time-of-Day Analysis (Are certain times more news-heavy?)\n",
    "news_df['hour'] = news_df['date'].dt.hour\n",
    "hourly_counts = news_df.groupby('hour')['headline'].count()\n",
    "\n",
    "print(\"\\nYearly News Article Counts:\\n\", yearly_counts)\n",
    "print(\"\\nMonthly News Article Counts:\\n\", monthly_counts)\n",
    "\n",
    "\n",
    "print(\"\\nHourly Distribution of News:\\n\", hourly_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bdb577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Publishers by News Count:\n",
      " publisher\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "Eddie Staley          57254\n",
      "Hal Lindon            49047\n",
      "ETF Professor         28489\n",
      "Juan Lopez            28438\n",
      "Benzinga Staff        28114\n",
      "Name: publisher, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "publisher_counts = news_df['publisher'].groupby(news_df['publisher']).count().sort_values(ascending=False)\n",
    "print(\"\\nTop Publishers by News Count:\\n\", publisher_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be14762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Common Words in Headlines:\n",
      " [('stocks', 161702), ('for', 154728), ('vs', 140965), ('in', 130298), ('eps', 128801), ('to', 124595), ('the', 122317), ('est', 122289), ('shares', 114140), ('reports', 108688)]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize headlines and remove stop words\n",
    "all_words = [word.lower() for headline in news_df['headline'] for word in word_tokenize(headline) \n",
    "             if word.isalpha() and word not in stop_words]\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Show top 10 most common words\n",
    "print(\"\\nTop 10 Most Common Words in Headlines:\\n\", word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f585176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Keywords from Financial News:\n",
      " ['52 week' 'adj eps' 'benzinga upgrades' 'companies trading'\n",
      " 'earnings scheduled' 'initiates coverage' 'market session'\n",
      " 'market update' 'mid day' 'pre market' 'price target' 'q1 eps' 'q2 eps'\n",
      " 'q3 eps' 'raises pt' 'stocks hit' 'stocks moving' 'trading higher'\n",
      " 'trading lower' 'vs est']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2,2), max_features=20)\n",
    "tfidf_matrix = vectorizer.fit_transform(news_df['headline'])\n",
    "\n",
    "# Display top phrases with highest TF-IDF scores\n",
    "print(\"\\nTop Keywords from Financial News:\\n\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda6cc85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Apply LDA model\u001b[39;00m\n\u001b[32m      5\u001b[39m lda_model = LatentDirichletAllocation(n_components=\u001b[32m5\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mlda_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorized_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Show top words per topic\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lda_model.components_):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:674\u001b[39m, in \u001b[36mLatentDirichletAllocation.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    666\u001b[39m         \u001b[38;5;28mself\u001b[39m._em_step(\n\u001b[32m    667\u001b[39m             X[idx_slice, :],\n\u001b[32m    668\u001b[39m             total_samples=n_samples,\n\u001b[32m    669\u001b[39m             batch_update=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    670\u001b[39m             parallel=parallel,\n\u001b[32m    671\u001b[39m         )\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    673\u001b[39m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i + \u001b[32m1\u001b[39m) % evaluate_every == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:523\u001b[39m, in \u001b[36mLatentDirichletAllocation._em_step\u001b[39m\u001b[34m(self, X, total_samples, batch_update, parallel)\u001b[39m\n\u001b[32m    496\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[32m    497\u001b[39m \n\u001b[32m    498\u001b[39m \u001b[33;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m \u001b[33;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m _, suff_stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:466\u001b[39m, in \u001b[36mLatentDirichletAllocation._e_step\u001b[39m\u001b[34m(self, X, cal_sstats, random_init, parallel)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    465\u001b[39m     parallel = Parallel(n_jobs=n_jobs, verbose=\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.verbose - \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[32m    480\u001b[39m doc_topics, sstats_list = \u001b[38;5;28mzip\u001b[39m(*results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\est_lul\\Documents\\Projects\\financial-sentiment-analysis\\venv\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:145\u001b[39m, in \u001b[36m_update_doc_distribution\u001b[39m\u001b[34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[39m\n\u001b[32m    141\u001b[39m last_d = doc_topic_d\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m norm_phi = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_doc_topic_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_topic_word_d\u001b[49m\u001b[43m)\u001b[49m + eps\n\u001b[32m    147\u001b[39m doc_topic_d = exp_doc_topic_d * np.dot(cnts / norm_phi, exp_topic_word_d.T)\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Convert text data to numerical format\n",
    "vectorized_data = vectorizer.fit_transform(news_df['headline'])\n",
    "\n",
    "# Apply LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(vectorized_data)\n",
    "\n",
    "# Show top words per topic\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"\\nTopic {idx+1}: \", [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count articles published per day\n",
    "daily_news_count = news_df.groupby('date').size().reset_index(name='num_articles')\n",
    "\n",
    "# Plot publication frequency over time\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(daily_news_count['date'], daily_news_count['num_articles'], label='Articles per Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Daily Financial News Publication Trend')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
